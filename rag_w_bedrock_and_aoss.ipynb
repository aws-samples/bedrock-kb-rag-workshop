{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2dc3fcb-ae4f-48e6-9b1c-71b002e0fe1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RAG with Amazon Bedrock Knowledge Base\n",
    "\n",
    "In this notebook we use the information ingested in the Bedrock knowledge base to answer user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d4975",
   "metadata": {},
   "source": [
    "## Import packages and utility functions\n",
    "Import packages, setup utility functions, interface with Amazon OpenSearch Service Serverless (AOSS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce61b6-795b-488c-b400-1ac80d355162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "from typing import Dict\n",
    "from urllib.request import urlretrieve\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from IPython.display import Markdown, display\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ea784-37bc-4a3f-84e3-1047f7e5cfd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global constants\n",
    "SERVICE = 'aoss'\n",
    "\n",
    "# do not change the name of the CFN stack, we assume that the \n",
    "# blog post creates a stack by this name and read output values\n",
    "# from the stack.\n",
    "CFN_STACK_NAME = \"rag-w-bedrock-kb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d559b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Anthropic models need the Human/Assistant terminology used in the prompts, \n",
    "# they work better with XML style tags.\n",
    "PROMPT_TEMPLATE = \"\"\"Human: Answer the question based only on the information provided in few sentences.\n",
    "<context>\n",
    "{}\n",
    "</context>\n",
    "Include your answer in the <answer></answer> tags. Do not include any preamble in your answer.\n",
    "<question>\n",
    "{}\n",
    "</question>\n",
    "Assistant:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6f5cc-2384-4f18-8add-418b258e8ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def get_cfn_outputs(stackname: str) -> str:\n",
    "    cfn = boto3.client('cloudformation')\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "def printmd(string: str):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c8d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions to talk to OpenSearch\n",
    "\n",
    "# Define queries for OpenSearch\n",
    "def query_docs(query: str, embeddings: BedrockEmbeddings, aoss_client: OpenSearch, index: str, k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Convert the query into embedding and then find similar documents from AOSS\n",
    "    \"\"\"\n",
    "\n",
    "    # embedding\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    # query to lookup OpenSearch kNN vector. Can add any metadata fields based filtering\n",
    "    # here as part of this query.\n",
    "    query_qna = {\n",
    "        \"size\": k,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "            \"vector\": {\n",
    "                \"vector\": query_embedding,\n",
    "                \"k\": k\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # OpenSearch API call\n",
    "    relevant_documents = aoss_client.search(\n",
    "        body = query_qna,\n",
    "        index = index\n",
    "    )\n",
    "    return relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d011b20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_context_for_query(q: str, embeddings: BedrockEmbeddings, aoss_client: OpenSearch, vector_index: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a context out of the similar docs retrieved from the vector database\n",
    "    by concatenating the text from the similar documents.\n",
    "    \"\"\"\n",
    "    print(f\"query -> {q}\")\n",
    "    aoss_response = query_docs(q, embeddings, aoss_client, vector_index)\n",
    "    context = \"\"\n",
    "    for r in aoss_response['hits']['hits']:\n",
    "        s = r['_source']\n",
    "        print(f\"{s['metadata']}\\n{s['text']}\")\n",
    "        context += f\"{s['text']}\\n\"\n",
    "        print(\"----------------\")\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf61b1",
   "metadata": {},
   "source": [
    "## Retrieve parameters needed from the AWS CloudFormation stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10051806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "outputs = get_cfn_outputs(CFN_STACK_NAME)\n",
    "\n",
    "region = outputs[\"Region\"]\n",
    "aoss_collection_arn = outputs['CollectionARN']\n",
    "aoss_host = f\"{os.path.basename(aoss_collection_arn)}.{region}.aoss.amazonaws.com\"\n",
    "aoss_vector_index = outputs['AOSSVectorIndexName']\n",
    "print(f\"aoss_collection_arn={aoss_collection_arn}\\naoss_host={aoss_host}\\naoss_vector_index={aoss_vector_index}\\naws_region={region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a5e9e",
   "metadata": {},
   "source": [
    "## Setup Embeddings and Text Generation model\n",
    "\n",
    "We can use LangChain to setup the embeddings and text generation models provided via Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6613d2-aae8-48e5-adfb-0ea7fb75f2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a boto3 bedrock client\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "# we will use Anthropic Claude for text generation\n",
    "claude_llm = Bedrock(model_id= \"anthropic.claude-v2\", client=bedrock_client)\n",
    "claude_llm.model_kwargs = dict(temperature=0.5, max_tokens_to_sample=300, top_k=250, top_p=1, stop_sequences=[])\n",
    "\n",
    "# we will be using the Titan Embeddings Model to generate our Embeddings.\n",
    "embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-g1-text-02\", client=bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f0166a",
   "metadata": {},
   "source": [
    "## Interface with Amazon OpenSearch Service Serverless\n",
    "We use the open-source [opensearch-py](https://pypi.org/project/opensearch-py/) package to talk to AOSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36f340-81ea-4617-b37d-57bf7669c9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, SERVICE)\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': aoss_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e383e23",
   "metadata": {},
   "source": [
    "## Use Retrieval Augumented Generation (RAG) for answering queries\n",
    "\n",
    "Now that we have setup the LLMs through Bedrock and vector database through AOSS, we are ready to answer queries using RAG. The workflow is as follows:\n",
    "\n",
    "1. Convert the user query into embeddings.\n",
    "\n",
    "1. Use the embeddings to find similar documents from the vector database.\n",
    "\n",
    "1. Create a prompt using the user query and similar documents (retrieved from the vector db) to create a prompt.\n",
    "\n",
    "1. Provide the prompt to the LLM to create an answer to the user query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0224f2c4-b725-4f3a-84ac-914c4eba8a94",
   "metadata": {},
   "source": [
    "## Query 1\n",
    "\n",
    "Let us first ask the our question to the model without providing any context, see the result and then ask the same question with context provided using document retrieved from AOSS and see if the answer improves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be3215-3dde-4abd-8c38-45871e63d058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Start with the query\n",
    "q = \"What versions of XGBoost are supported by Amazon SageMaker?\"\n",
    "\n",
    "# 2. Now create a prompt by combining the query and the context (which is empty at this time)\n",
    "context = \"\"\n",
    "prompt = PROMPT_TEMPLATE.format(context, q)\n",
    "\n",
    "# 3. Provide the prompt to the LLM to generate an answer to the query without any additional context provided\n",
    "response = claude_llm(prompt)\n",
    "printmd(f\"<span style='color:red'><b>question={q.strip()}<br>answer={response.strip()}</b></span>\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f429bb-050d-4c81-b532-aa5b8e531990",
   "metadata": {},
   "source": [
    "**The answer provided above is incorrect**, as can be seen from the [SageMaker XGBoost Algorithm page](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html). The supported version numbers are \"1.0, 1.2, 1.3, 1.5, and 1.7\".\n",
    "\n",
    "Now, let us see if we can improve upon this answer by using additional information that is available to use in the vector database. **Also notice in the response below that the source of the documents that are being used as context is also being called out (the name of the file in the S3 bucket), this helps create confidence in the response generated by the LLM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f86e8-157f-41b0-88a4-59a56f5507c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Start with the query\n",
    "q = \"What versions of XGBoost are supported by Amazon SageMaker?\"\n",
    "\n",
    "# 2. Create the context by finding similar documents from the knowledge base\n",
    "context = create_context_for_query(q, embeddings, client, aoss_vector_index)\n",
    "\n",
    "# 3. Now create a prompt by combining the query and the context\n",
    "prompt = PROMPT_TEMPLATE.format(context, q)\n",
    "\n",
    "# 4. Provide the prompt to the LLM to generate an answer to the query based on context provided\n",
    "response = claude_llm(prompt)\n",
    "\n",
    "printmd(f\"<span style='color:red'><b>question={q.strip()}<br>answer={response.strip()}</b></span>\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1bd68-f61d-4f15-b152-3f9f54305fa8",
   "metadata": {},
   "source": [
    "## Query 2\n",
    "\n",
    "For the subsequent queries we use RAG directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffbe92d-5fcd-480d-a239-0c461f61f4a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Start with the query\n",
    "q = \"What are the different types of distributed training supported by SageMaker. Give a short summary of each.\"\n",
    "\n",
    "# 2. Create the context by finding similar documents from the knowledge base\n",
    "context = create_context_for_query(q, embeddings, client, aoss_vector_index)\n",
    "\n",
    "# 3. Now create a prompt by combining the query and the context\n",
    "prompt = PROMPT_TEMPLATE.format(context, q)\n",
    "\n",
    "# 4. Provide the prompt to the LLM to generate an answer to the query based on context provided\n",
    "response = claude_llm(prompt)\n",
    "printmd(f\"<span style='color:red'><b>question={q.strip()}<br>answer={response.strip()}</b></span>\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8024b1f-3f99-406c-be1d-9368cd1440f4",
   "metadata": {},
   "source": [
    "## Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444ae8c-0377-46ad-8d4e-2d41f575c289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Start with the query\n",
    "q = \"What advantages does SageMaker debugger provide?\"\n",
    "\n",
    "# 2. Create the context by finding similar documents from the knowledge base\n",
    "context = create_context_for_query(q, embeddings, client, aoss_vector_index)\n",
    "\n",
    "# 3. Now create a prompt by combining the query and the context\n",
    "prompt = PROMPT_TEMPLATE.format(context, q)\n",
    "\n",
    "# 4. Provide the prompt to the LLM to generate an answer to the query based on context provided\n",
    "response = claude_llm(prompt)\n",
    "\n",
    "printmd(f\"<span style='color:red'><b>question={q.strip()}<br>answer={response.strip()}</b></span>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7ac93-f5ed-4c0c-99bf-03fa1ab7cf7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('bedrock_py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ac4445fedcc02e0ec010c021cc980cd9c85bdedf3d57447a4cb4e8d37edc5f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
