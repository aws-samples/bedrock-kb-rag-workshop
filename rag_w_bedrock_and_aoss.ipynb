{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2dc3fcb-ae4f-48e6-9b1c-71b002e0fe1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RAG with Amazon Bedrock Knowledge Base\n",
    "\n",
    "In this notebook we use the information ingested in the Bedrock knowledge base to answer user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d4975",
   "metadata": {},
   "source": [
    "## Import packages and utility functions\n",
    "Import packages, setup utility functions, interface with Amazon OpenSearch Service Serverless (AOSS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85ce61b6-795b-488c-b400-1ac80d355162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "from typing import Dict\n",
    "from urllib.request import urlretrieve\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from IPython.display import Markdown, display\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c1ea784-37bc-4a3f-84e3-1047f7e5cfd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global constants\n",
    "SERVICE = 'aoss'\n",
    "\n",
    "# do not change the name of the CFN stack, we assume that the \n",
    "# blog post creates a stack by this name and read output values\n",
    "# from the stack.\n",
    "CFN_STACK_NAME = \"rag-w-bedrock-kb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d559b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Anthropic models need the Human/Assistant terminology used in the prompts, \n",
    "# they work better with XML style tags.\n",
    "PROMPT_TEMPLATE = \"\"\"Human: Answer the question based only on the information provided in few sentences.\n",
    "<context>\n",
    "{}\n",
    "</context>\n",
    "Include your answer in the <answer></answer> tags. Do not include any preamble in your answer.\n",
    "<question>\n",
    "{}\n",
    "</question>\n",
    "Assistant:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61c6f5cc-2384-4f18-8add-418b258e8ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def get_cfn_outputs(stackname: str) -> str:\n",
    "    cfn = boto3.client('cloudformation')\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "def printmd(string: str):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "326c8d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions to talk to OpenSearch\n",
    "\n",
    "# Define queries for OpenSearch\n",
    "def query_docs(query: str, embeddings: BedrockEmbeddings, aoss_client: OpenSearch, index: str, k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Convert the query into embedding and then find similar documents from AOSS\n",
    "    \"\"\"\n",
    "\n",
    "    # embedding\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    # query to lookup OpenSearch kNN vector. Can add any metadata fields based filtering\n",
    "    # here as part of this query.\n",
    "    query_qna = {\n",
    "        \"size\": k,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "            \"vector\": {\n",
    "                \"vector\": query_embedding,\n",
    "                \"k\": k\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # OpenSearch API call\n",
    "    relevant_documents = aoss_client.search(\n",
    "        body = query_qna,\n",
    "        index = index\n",
    "    )\n",
    "    return relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d011b20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_context_for_query(q: str, embeddings: BedrockEmbeddings, aoss_client: OpenSearch, vector_index: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a context out of the similar docs retrieved from the vector database\n",
    "    by concatenating the text from the similar documents.\n",
    "    \"\"\"\n",
    "    print(f\"query -> {q}\")\n",
    "    aoss_response = query_docs(q, embeddings, aoss_client, vector_index)\n",
    "    context = \"\"\n",
    "    for r in aoss_response['hits']['hits']:\n",
    "        s = r['_source']\n",
    "        print(f\"{s['metadata']}\\n{s['text']}\")\n",
    "        context += f\"{s['text']}\\n\"\n",
    "        print(\"----------------\")\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf61b1",
   "metadata": {},
   "source": [
    "## Retrieve parameters needed from the AWS CloudFormation stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10051806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aoss_collection_arn=arn:aws:aoss:us-east-1:015469603702:collection/mi4dut5xaxie0wkxe8yj\n",
      "aoss_host=mi4dut5xaxie0wkxe8yj.us-east-1.aoss.amazonaws.com\n",
      "aoss_vector_index=sagemaker-readthedocs-io\n",
      "aws_region=us-east-1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = get_cfn_outputs(CFN_STACK_NAME)\n",
    "\n",
    "region = outputs[\"Region\"]\n",
    "aoss_collection_arn = outputs['CollectionARN']\n",
    "aoss_host = f\"{os.path.basename(aoss_collection_arn)}.{region}.aoss.amazonaws.com\"\n",
    "aoss_vector_index = outputs['AOSSVectorIndexName']\n",
    "print(f\"aoss_collection_arn={aoss_collection_arn}\\naoss_host={aoss_host}\\naoss_vector_index={aoss_vector_index}\\naws_region={region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a5e9e",
   "metadata": {},
   "source": [
    "## Setup Embeddings and Text Generation model\n",
    "\n",
    "We can use LangChain to setup the embeddings and text generation models provided via Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf6613d2-aae8-48e5-adfb-0ea7fb75f2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we will use Anthropic Claude for text generation\n",
    "claude_llm = Bedrock(model_id= \"anthropic.claude-v2\")\n",
    "claude_llm.model_kwargs = dict(temperature=0.5, max_tokens_to_sample=300, top_k=250, top_p=1, stop_sequences=[])\n",
    "\n",
    "# we will be using the Titan Embeddings Model to generate our Embeddings.\n",
    "embeddings = BedrockEmbeddings(model_id = \"amazon.titan-embed-g1-text-02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f0166a",
   "metadata": {},
   "source": [
    "## Interface with Amazon OpenSearch Service Serverless\n",
    "We use the open-source [opensearch-py](https://pypi.org/project/opensearch-py/) package to talk to AOSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d36f340-81ea-4617-b37d-57bf7669c9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, SERVICE)\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': aoss_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e383e23",
   "metadata": {},
   "source": [
    "## Use Retrieval Augumented Generation (RAG) for answering queries\n",
    "\n",
    "Now that we have setup the LLMs through Bedrock and vector database through AOSS, we are ready to answer queries using RAG. The workflow is as follows:\n",
    "\n",
    "1. Convert the user query into embeddings.\n",
    "\n",
    "1. Use the embeddings to find similar documents from the vector database.\n",
    "\n",
    "1. Create a prompt using the user query and similar documents (retrieved from the vector db) to create a prompt.\n",
    "\n",
    "1. Provide the prompt to the LLM to create an answer to the user query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0224f2c4-b725-4f3a-84ac-914c4eba8a94",
   "metadata": {},
   "source": [
    "## Query 1\n",
    "\n",
    "Let us first ask the our question to the model without providing any context, see the result and then ask the same question with context provided using document retrieved from AOSS and see if the answer improves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4be3215-3dde-4abd-8c38-45871e63d058",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'><b>question=What versions of XGBoost are supported by Amazon SageMaker?<br>answer=<answer>\n",
       "Amazon SageMaker supports XGBoost versions 0.90-1, 0.90-2, and 1.0-1.\n",
       "</answer></b></span>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Start with the query\n",
    "q = \"What versions of XGBoost are supported by Amazon SageMaker?\"\n",
    "\n",
    "# 2. Now create a prompt by combining the query and the context (which is empty at this time)\n",
    "context = \"\"\n",
    "prompt = PROMPT_TEMPLATE.format(context, q)\n",
    "\n",
    "# 3. Provide the prompt to the LLM to generate an answer to the query without any additional context provided\n",
    "response = claude_llm(prompt)\n",
    "printmd(f\"<span style='color:red'><b>question={q.strip()}<br>answer={response.strip()}</b></span>\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f429bb-050d-4c81-b532-aa5b8e531990",
   "metadata": {},
   "source": [
    "**The answer provided above is incorrect**, as can be seen from the [SageMaker XGBoost Algorithm page](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html). The supported version numbers are \"1.0, 1.2, 1.3, 1.5, and 1.7\".\n",
    "\n",
    "Now, let us see if we can improve upon this answer by using additional information that is available to use in the vector database. **Also notice in the response below that the source of the documents that are being used as context is also being called out (the name of the file in the S3 bucket), this helps create confidence in the response generated by the LLM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "371f86e8-157f-41b0-88a4-59a56f5507c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query -> What versions of XGBoost are supported by Amazon SageMaker?\n",
      "{\"source\":\"s3://sagemaker-kb-015469603702/sagemaker.readthedocs.io_en_stable_frameworks_xgboost_using_xgboost.html\"}\n",
      "see Extending our PyTorch containers. Use XGBoost as a Built-in Algortihm¶ Amazon SageMaker provides XGBoost as a built-in algorithm that you can use like other built-in algorithms. Using the built-in algorithm version of XGBoost is simpler than using the open source version, because you don’t have to write a training script. If you don’t need the features and flexibility of open source XGBoost, consider using the built-in version. For information about using the Amazon SageMaker XGBoost built-in algorithm, see XGBoost Algorithm in the Amazon SageMaker Developer Guide. Use the Open Source XGBoost Algorithm¶ If you want the flexibility and additional features that it provides, use the SageMaker open source XGBoost algorithm. For which XGBoost versions are supported, see the AWS documentation. We recommend that you use the latest supported version because that’s where we focus most of our development efforts. For a complete example of using the open source XGBoost algorithm, see the sample notebook at https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone_dist_script_mode.ipynb. For more information about XGBoost, see the XGBoost documentation. Train a Model with Open Source XGBoost¶ To train a model by using the Amazon SageMaker open source XGBoost algorithm: Prepare a training script Create a sagemaker.xgboost.XGBoost estimator Call the estimator’s fit method Prepare a Training Script¶ A typical training script loads data from the input channels, configures training with hyperparameters, trains a model,\n",
      "----------------\n",
      "{\"source\":\"s3://sagemaker-kb-015469603702/sagemaker.readthedocs.io_en_stable_frameworks_xgboost_using_xgboost.html\"}\n",
      "Models with Multi-Model Endpoints SageMaker XGBoost Classes SageMaker XGBoost Docker Containers eXtreme Gradient Boosting (XGBoost) is a popular and efficient machine learning algorithm used for regression and classification tasks on tabular datasets. It implements a technique known as gradient boosting on trees, which performs remarkably well in machine learning competitions. Amazon SageMaker supports two ways to use the XGBoost algorithm: XGBoost built-in algorithm XGBoost open source algorithm The XGBoost open source algorithm provides the following benefits over the built-in algorithm: Latest version - The open source XGBoost algorithm typically supports a more recent version of XGBoost. To see the XGBoost version that is currently supported, see XGBoost SageMaker Estimators and Models. Flexibility - Take advantage of the full range of XGBoost functionality, such as cross-validation support. You can add custom pre- and post-processing logic and run additional code after training. Scalability - The XGBoost open source algorithm has a more efficient implementation of distributed training, which enables it to scale out to more instances and reduce out-of-memory errors. Extensibility - Because the open source XGBoost container is open source, you can extend the container to install additional libraries and change the version of XGBoost that the container uses. For an example notebook that shows how to extend SageMaker containers, see Extending our PyTorch containers. Use XGBoost as a Built-in Algortihm¶ Amazon SageMaker provides XGBoost as a built-in algorithm that you can use like other built-in algorithms. Using the built-in algorithm version of XGBoost is simpler than using the open source version, because you don’t have to write\n",
      "----------------\n",
      "{\"source\":\"s3://sagemaker-kb-015469603702/sagemaker.readthedocs.io_en_stable_algorithms_tabular_xgboost.html\"}\n",
      "an expanded set of metrics than the original versions. It provides an XGBoost estimator that executes a training script in a managed XGBoost environment. The current release of SageMaker XGBoost is based on the original XGBoost versions 1.0, 1.2, 1.3, and 1.5. The following table outlines a variety of sample notebooks that address different use cases of Amazon SageMaker XGBoost algorithm. Notebook Title Description How to Create a Custom XGBoost container? This notebook shows you how to build a custom XGBoost Container with Amazon SageMaker Batch Transform. Regression with XGBoost using Parquet This notebook shows you how to use the Abalone dataset in Parquet to train a XGBoost model. How to Train and Host a Multiclass Classification Model? This notebook shows how to use the MNIST dataset to train and host a multiclass classification model. How to train a Model for Customer Churn Prediction? This notebook shows you how to train a model to Predict Mobile Customer Departure in an effort to identify unhappy customers. An Introduction to Amazon SageMaker Managed Spot infrastructure for XGBoost Training This notebook shows you how to use Spot Instances for training with a XGBoost Container. How to use Amazon SageMaker Debugger to debug XGBoost Training Jobs? This notebook shows you how to use Amazon SageMaker Debugger to monitor training jobs to detect inconsistencies. How to use Amazon SageMaker Debugger to debug XGBoost Training Jobs in Real-Time? This notebook shows you how to use the MNIST dataset and Amazon SageMaker Debugger to perform real-time analysis of XGBoost training jobs while training jobs are running. For instructions on how to create and access Jupyter\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'><b>question=What versions of XGBoost are supported by Amazon SageMaker?<br>answer=<answer>\n",
       "The XGBoost open source algorithm supports the latest XGBoost version. The XGBoost built-in algorithm is based on XGBoost versions 1.0, 1.2, 1.3, and 1.5.\n",
       "</answer></b></span>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Start with the query\n",
    "q = \"What versions of XGBoost are supported by Amazon SageMaker?\"\n",
    "\n",
    "# 2. Create the context by finding similar documents from the knowledge base\n",
    "context = create_context_for_query(q, embeddings, client, aoss_vector_index)\n",
    "\n",
    "# 3. Now create a prompt by combining the query and the context\n",
    "prompt = PROMPT_TEMPLATE.format(context, q)\n",
    "\n",
    "# 4. Provide the prompt to the LLM to generate an answer to the query based on context provided\n",
    "response = claude_llm(prompt)\n",
    "\n",
    "printmd(f\"<span style='color:red'><b>question={q.strip()}<br>answer={response.strip()}</b></span>\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1bd68-f61d-4f15-b152-3f9f54305fa8",
   "metadata": {},
   "source": [
    "## Query 2\n",
    "\n",
    "For the subsequent queries we use RAG directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ffbe92d-5fcd-480d-a239-0c461f61f4a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query -> What are the different types of distributed training supported by SageMaker. Give a short summary of each.\n",
      "{\"source\":\"s3://sagemaker-kb-015469603702/sagemaker.readthedocs.io_en_stable_api_training_distributed.html\"}\n",
      "Archive Launch a Distributed Training Job Using the SageMaker Python SDK Release Notes SageMaker Distributed Data Parallel 1.8.0 Release Notes Release History The SageMaker Distributed Model Parallel Library¶ The SageMaker Distributed Model Parallel Library Overview Use the Library’s API to Adapt Training Scripts Version 1.11.0, 1.13.0, 1.14.0, 1.15.0 (Latest) Documentation Archive Run a Distributed Training Job Using the SageMaker Python SDK Configuration Parameters for distribution Ranking Basics without Tensor Parallelism Placement Strategy with Tensor Parallelism Prescaled Batch Release Notes SageMaker Distributed Model Parallel 1.15.0 Release Notes Release History Next Previous © Copyright 2023, Amazon Revision af4d7949. Built with Sphinx using a theme provided by Read the Docs. Read the Docs v: stable Versions stable v2.167.0 v2.166.0 v2.165.0 v2.164.0 v2.163.0 v2.162.0 v2.161.0 v2.160.0 v2.159.0 v2.158.0 v2.157.0 v2.156.0 v2.155.0 v2.154.0 v2.153.0 v2.152.0 v2.151.0 v2.150.0 v2.149.0 v2.148.0 v2.147.0 v2.146.1 v2.146.0 v2.145.0 v2.144.0 v2.143.0 v2.142.0 v2.141.0 v2.140.1 v2.140.0\n",
      "----------------\n",
      "{\"source\":\"s3://sagemaker-kb-015469603702/sagemaker.readthedocs.io_en_stable_api_training_distributed.html\"}\n",
      "sagemaker stable Filters: Example Dev Guide SDK Guide Using the SageMaker Python SDK Use Version 2.x of the SageMaker Python SDK APIs Feature Store APIs Training APIs Distributed Training APIs The SageMaker Distributed Data Parallel Library The SageMaker Distributed Data Parallel Library Overview Use the Library to Adapt Your Training Script Launch a Distributed Training Job Using the SageMaker Python SDK Release Notes The SageMaker Distributed Model Parallel Library The SageMaker Distributed Model Parallel Library Overview Use the Library’s API to Adapt Training Scripts Run a Distributed Training Job Using the SageMaker Python SDK Release Notes Inference APIs Governance APIs Utility APIs Frameworks Built-in Algorithms Workflows Amazon SageMaker Experiments Amazon SageMaker Debugger Amazon SageMaker Feature Store Amazon SageMaker Model Monitor Amazon SageMaker Processing Amazon SageMaker Model Building Pipeline sagemaker » APIs » Distributed Training APIs Edit on GitHub Distributed Training APIs¶ SageMaker distributed training libraries offer both data parallel and model parallel training strategies. They combine software and hardware technologies to improve inter-GPU and inter-node communications. They extend SageMaker’s training capabilities with built-in options that require only small code changes to your training scripts. The SageMaker Distributed Data Parallel Library¶ The SageMaker Distributed Data Parallel Library Overview Use the Library to Adapt Your Training Script For versions between 1.4.0 and 1.8.0 (Latest) Documentation Archive Launch a Distributed Training Job Using the SageMaker Python SDK Release Notes SageMaker Distributed Data Parallel 1.8.0 Release Notes Release History The SageMaker Distributed Model Parallel Library¶ The SageMaker Distributed Model Parallel Library Overview Use the Library’s API to Adapt Training Scripts Version 1.11.0, 1.13.0,\n",
      "----------------\n",
      "{\"source\":\"s3://sagemaker-kb-015469603702/sagemaker.readthedocs.io_en_stable_amazon_sagemaker_debugger.html\"}\n",
      "training by calling fit # Setting the wait to `False` would make the fit asynchronous estimator.fit(wait=False) # Get a list of S3 URIs S3Downloader.list(estimator.latest_job_debugger_artifacts_path()) Continuous analyses through rules¶ In addition to collecting the debugging data, Amazon SageMaker Debugger provides the capability for you to analyze it in a streaming fashion using “rules”. A SageMaker Debugger “rule” is a piece of code which encapsulates the logic for analyzing debugging data. SageMaker Debugger provides a set of built-in rules curated by data scientists and engineers at Amazon to identify common problems while training machine learning models. There is also support for using custom rule source codes for evaluation. In the following sections, you’ll learn how to use both the built-in and custom rules while training your model. Relationship between debugger hook and rules¶ Using SageMaker Debugger is, broadly, a two-pronged approach. On one hand you have the production of debugging data, which is done through the Debugger Hook, and on the other hand you have the consumption of this data, which can be with rules (for continuous analyses) or by using the SageMaker Debugger SDK (for interactive analyses). The production and consumption of data are defined independently. For example, you could configure the debugging hook to store only the collection “gradients” and then configure the rules to operate on some other collection, say, “weights”. While this is possible, it’s quite useless as it gives you no meaningful insight into the training process. This is because the rule will do nothing in this example scenario since it will wait\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'><b>question=What are the different types of distributed training supported by SageMaker. Give a short summary of each.<br>answer=<answer>\n",
       "SageMaker supports two main types of distributed training:\n",
       "\n",
       "1. SageMaker Distributed Data Parallel: This allows scaling training by splitting data across multiple instances. It uses data parallelism to train models faster.\n",
       "\n",
       "2. SageMaker Distributed Model Parallel: This allows scaling training by splitting models across multiple instances. It uses model parallelism to train very large models that cannot fit on a single instance.\n",
       "</answer></b></span>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Start with the query\n",
    "q = \"What are the different types of distributed training supported by SageMaker. Give a short summary of each.\"\n",
    "\n",
    "# 2. Create the context by finding similar documents from the knowledge base\n",
    "context = create_context_for_query(q, embeddings, client, aoss_vector_index)\n",
    "\n",
    "# 3. Now create a prompt by combining the query and the context\n",
    "prompt = PROMPT_TEMPLATE.format(context, q)\n",
    "\n",
    "# 4. Provide the prompt to the LLM to generate an answer to the query based on context provided\n",
    "response = claude_llm(prompt)\n",
    "printmd(f\"<span style='color:red'><b>question={q.strip()}<br>answer={response.strip()}</b></span>\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8024b1f-3f99-406c-be1d-9368cd1440f4",
   "metadata": {},
   "source": [
    "## Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5444ae8c-0377-46ad-8d4e-2d41f575c289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query -> What advantages does SageMaker debugger provide?\n",
      "{\"source\":\"s3://sagemaker-kb-015469603702/sagemaker.readthedocs.io_en_stable_amazon_sagemaker_debugger.html\"}\n",
      "having the TensorBoard data emitted from the hook in addition to the tensors will incur a cost to the training and may slow it down. Interactive analysis using SageMaker Debugger SDK and visualizations¶ Amazon SageMaker Debugger SDK also allows you to do interactive analyses on the debugging data produced from a training job run and to render visualizations of it. After calling fit() on the estimator, you can use the SDK to load the saved data in a SageMaker Debugger trial and do an analysis on the data: from smdebug.trials import create_trial s3_output_path = estimator.latest_job_debugger_artifacts_path() trial = create_trial(s3_output_path) To learn more about the programming model for analysis using the SageMaker Debugger SDK, see SageMaker Debugger Analysis. For a tutorial on what you can do after creating the trial and how to visualize the results, see SageMaker Debugger - Visualizing Debugging Results. Default behavior and opting out¶ For TensorFlow, Keras, MXNet, PyTorch and XGBoost estimators, the DebuggerHookConfig is always initialized regardless of specification while initializing the estimator. This is done to minimize code changes needed to get useful debugging information. To disable the hook initialization, you can do so by specifying False for value of debugger_hook_config in your framework estimator’s initialization: estimator = TensorFlow( role=role, instance_count=1, instance_type=instance_type, debugger_hook_config=False ) Learn More¶ Further documentation¶ API documentation: https://sagemaker.readthedocs.io/en/stable/debugger.html AWS\n",
      "----------------\n",
      "{\"source\":\"s3://sagemaker-kb-015469603702/sagemaker.readthedocs.io_en_stable_amazon_sagemaker_debugger.html\"}\n",
      "debugging hook to store only the collection “gradients” and then configure the rules to operate on some other collection, say, “weights”. While this is possible, it’s quite useless as it gives you no meaningful insight into the training process. This is because the rule will do nothing in this example scenario since it will wait for the tensors in the collection “gradients” which are never be emitted. For more useful and efficient debugging, configure your debugging hook to produce and store the debugging data that you care about and employ rules that operate on that particular data. This way, you ensure that the Debugger is utilized to its maximum potential in detecting anomalies. In this sense, there is a loose binding between the hook and the rules. Normally, you’d achieve this binding for a training job by providing values for both debugger_hook_config and rules in your estimator. However, SageMaker Debugger simplifies this by allowing you to specify the collection configuration within the Rule object itself. This way, you don’t have to specify the debugger_hook_config in your estimator separately. Using built-in rules¶ SageMaker Debugger comes with a set of built-in rules which can be used to identify common problems in model training, for example vanishing gradients or exploding tensors. You can choose to evaluate one or more of these rules while training your model to obtain meaningful insight into the training process. To learn more about these built in rules, see SageMaker Debugger Built-in Rules. Pre-defined debugger hook configuration for built-in rules¶ As mentioned earlier, for efficient\n",
      "----------------\n",
      "{\"source\":\"s3://sagemaker-kb-015469603702/sagemaker.readthedocs.io_en_stable_amazon_sagemaker_debugger.html\"}\n",
      "Specifying configurations for collections Collection Name Collection Parameters Hook Parameters Begin model training Continuous analyses through rules Relationship between debugger hook and rules Using built-in rules Pre-defined debugger hook configuration for built-in rules Sample Usages Using custom rules Sample Usage Capture real-time TensorBoard data from the debugging hook Interactive analysis using SageMaker Debugger SDK and visualizations Default behavior and opting out Learn More Further documentation Notebook examples Background¶ Amazon SageMaker provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. Amazon SageMaker is a fully-managed service that encompasses the entire machine learning workflow. You can label and prepare your data, choose an algorithm, train a model, and then tune and optimize it for deployment. You can deploy your models to production with Amazon SageMaker to make predictions at lower costs than was previously possible. SageMaker Debugger provides a way to hook into the training process and emit debug artifacts (a.k.a. “tensors”) that represent the training state at each point in the training lifecycle. Debugger then stores the data in real time and uses rules that encapsulate logic to analyze tensors and react to anomalies. Debugger provides built-in rules and allows you to write custom rules for analysis. Setup¶ To get started, you must satisfy the following prerequisites: Specify an AWS Region where you’ll train your model. Give Amazon SageMaker the access to your data in Amazon Simple Storage Service (Amazon S3) needed to train your model by creating an IAM role ARN. See the AWS IAM documentation for how to fine tune the permissions needed. Capture\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:red'><b>question=What advantages does SageMaker debugger provide?<br>answer=<answer>\n",
       "SageMaker Debugger provides the following advantages:\n",
       "\n",
       "- Allows you to hook into the training process and emit debug artifacts (tensors) that represent the training state. This provides visibility into the training process.\n",
       "\n",
       "- Stores the debug data in real time and allows you to analyze it using built-in rules or custom rules to detect anomalies. This enables debugging and monitoring of training. \n",
       "\n",
       "- Provides pre-defined debugger configurations for built-in rules, making it easier to debug common issues.\n",
       "\n",
       "- Captures real-time TensorBoard data for interactive analysis and visualization using the Debugger SDK.\n",
       "\n",
       "- Integrates with SageMaker training workflows with minimal code changes needed.\n",
       "\n",
       "</answer></b></span>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Start with the query\n",
    "q = \"What advantages does SageMaker debugger provide?\"\n",
    "\n",
    "# 2. Create the context by finding similar documents from the knowledge base\n",
    "context = create_context_for_query(q, embeddings, client, aoss_vector_index)\n",
    "\n",
    "# 3. Now create a prompt by combining the query and the context\n",
    "prompt = PROMPT_TEMPLATE.format(context, q)\n",
    "\n",
    "# 4. Provide the prompt to the LLM to generate an answer to the query based on context provided\n",
    "response = claude_llm(prompt)\n",
    "\n",
    "printmd(f\"<span style='color:red'><b>question={q.strip()}<br>answer={response.strip()}</b></span>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7ac93-f5ed-4c0c-99bf-03fa1ab7cf7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_bedrock_py39",
   "language": "python",
   "name": "conda_bedrock_py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
